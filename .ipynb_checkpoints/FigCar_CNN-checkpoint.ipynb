{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arvind/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import h5py \n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/arvind/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 16, 128, 128,   3], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.truncated_normal([16,128,128,3])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   16, 49152], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=tf.reshape(a,[16,49152])\n",
    "sess.run(tf.shape(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location of Data /home/arvind/Documents/Data/\n",
      "Created Test folder and successfully stored test images\n",
      "Instantiated DB\n",
      "car_ims/000001.jpg\n",
      "car_ims/000002.jpg\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e2e5e07be46a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mfile_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Location of Data \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mgroup_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-e2e5e07be46a>\u001b[0m in \u001b[0;36mgroup_images\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import scipy.io as spio\n",
    "def group_images(path):\n",
    "    \"\"\"Groups and renames images per car model in appropriate folders specified by path\"\"\"\n",
    "    \n",
    "    path2 = path + 'Train/'\n",
    "    path3 = path + 'Validation/'\n",
    "    path4 = path + 'Test/'\n",
    "    if not os.path.exists(path4):\n",
    "        os.makedirs(path4)\n",
    "    test_tar_location = path + 'cars_test/'\n",
    "    if os.path.exists(test_tar_location):\n",
    "        os.rename(test_tar_location, path + 'Test/cars_test/')\n",
    "    \"\"\"Verifies path creation\"\"\"\n",
    "    print(\"Created Test folder and successfully stored test images\")\n",
    "\n",
    "    mat = spio.loadmat(path + 'cars_annos.mat', squeeze_me=True)\n",
    "    annotations = mat['annotations']\n",
    "    class_names = mat['class_names']\n",
    "    print(\"Instantiated DB\")\n",
    "    \n",
    "    x = 0\n",
    "    for i in range((len(annotations[0]))):\n",
    "        car_name = class_names[0]\n",
    "        car_name = car_name[:-5].replace(\" \", \"_\")\n",
    "        if car_name == 'Ram_C/V_Cargo_Van_Minivan':\n",
    "            car_name = 'Ram_C_MiniVan'\n",
    "        newpath = path2 + car_name\n",
    "        validation_path = path3 + car_name\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "            os.makedirs(validation_path)\n",
    "            x = 0\n",
    "        src = path + annotations[i][0]\n",
    "        x = x + 1\n",
    "        try:\n",
    "            if(x == 1 or x == 2):\n",
    "                os.rename(src, validation_path + '/'+ car_name + str(x) + '.jpg')\n",
    "            else:\n",
    "                os.rename(src, newpath + '/'+ car_name + str(x-2) + '.jpg')\n",
    "        except:\n",
    "            pass\n",
    "    print(\"Train and Validation created successfully\")\n",
    "        \n",
    "file_location = input(\"Location of Data \")\n",
    "group_images(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    car_files = np.array(data['filenames'])\n",
    "    car_targets = np_utils.to_categorical(np.array(data['target']), 189)\n",
    "    return car_files, car_targets\n",
    "\n",
    "train_files, train_targets = load_dataset(file_location + 'Train')\n",
    "valid_files, valid_targets = load_dataset(file_location + 'Validation')\n",
    "test_files, test_targets = load_dataset(file_location + 'Test')\n",
    "\n",
    "car_names = [item[20:-1] for item in sorted(glob(file_location + \"Train/*/\"))]\n",
    "\n",
    "print('There are %d total car categories.' % len(car_names))\n",
    "print('There are %s total car images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training car images.' % len(train_files))\n",
    "print('There are %d validation car images.' % len(valid_files))\n",
    "print('There are %d test car images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "def path_to_tensor(img_path):     \n",
    "    # loads RGB image as PIL.Image.Image type     \n",
    "    img = image.load_img(img_path, target_size=(224, 224))     \n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)     \n",
    "    x = image.img_to_array(img)     \n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor     \n",
    "    return np.expand_dims(x, axis=0)  \n",
    "def paths_to_tensor(img_paths):     \n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]     \n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
