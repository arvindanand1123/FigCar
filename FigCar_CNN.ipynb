{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-efab23cde202>:3: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 16, 128, 128,   3], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.truncated_normal([16,128,128,3])\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "sess.run(tf.shape(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   16, 49152], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=tf.reshape(a,[16,49152])\n",
    "sess.run(tf.shape(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location of Data //Users//arvindanand//Documents\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    //Users//arvindanand//Documents\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def group_images(path):\n",
    "    \"\"\"Groups and renames images per car model in appropriate folders specified by path\"\"\"\n",
    "    \n",
    "    path2 = path + 'Train/'\n",
    "    path3 = path + 'Validation/'\n",
    "    path4 = path + 'Test/'\n",
    "    if not os.path.exists(path4):\n",
    "        os.makedirs(path4)\n",
    "    test_tar_location = path + 'cars_test/'\n",
    "    if os.path.exists(test_tar_location):\n",
    "        os.rename(test_tar_location, path + 'Test/cars_test/')\n",
    "    \"\"\"Verifies path creation\"\"\"\n",
    "    print(\"Created Test folder and successfully stored test images\")\n",
    "    \n",
    "    mat = scipy.io.loadmat(path + 'cars_annos.mat')\n",
    "    annotations = mat['annotations']\n",
    "    class_names = mat['class_names']\n",
    "    print(\"Instantiated DB\")\n",
    "    \n",
    "    x = 0\n",
    "    for i in range((len(annotations[0]))):\n",
    "        car_name = class_names[0][annotations[0][i][5][0][0]-1][0]\n",
    "        car_name = car_name[:-5].replace(\" \", \"_\")\n",
    "        if car_name == 'Ram_C/V_Cargo_Van_Minivan':\n",
    "            car_name = 'Ram_C_MiniVan'\n",
    "        newpath = path2 + car_name\n",
    "        validation_path = path3 + car_name\n",
    "        if not os.path.exists(newpath):\n",
    "            os.makedirs(newpath)\n",
    "            os.makedirs(validation_path)\n",
    "            x = 0\n",
    "        src = path + annotations[0][i][0][0]\n",
    "        x = x + 1\n",
    "        try:\n",
    "            if(x == 1 or x == 2):\n",
    "                os.rename(src, validation_path + '/'+ car_name + str(x) + '.jpg')\n",
    "            else:\n",
    "                os.rename(src, newpath + '/'+ car_name + str(x-2) + '.jpg')\n",
    "        except:\n",
    "            pass\n",
    "    print(\"Train and Validation created successfully\")\n",
    "        \n",
    "file_location = input(\"Location of Data \")\n",
    "group_images(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_location' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1ea38f89656c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcar_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcar_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mvalid_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_location' is not defined"
     ]
    }
   ],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    car_files = np.array(data['filenames'])\n",
    "    car_targets = np_utils.to_categorical(np.array(data['target']), 189)\n",
    "    return car_files, car_targets\n",
    "\n",
    "train_files, train_targets = load_dataset(file_location + 'Train')\n",
    "valid_files, valid_targets = load_dataset(file_location + 'Validation')\n",
    "test_files, test_targets = load_dataset(file_location + 'Test')\n",
    "\n",
    "car_names = [item[20:-1] for item in sorted(glob(file_location + \"Train/*/\"))]\n",
    "\n",
    "print('There are %d total car categories.' % len(car_names))\n",
    "print('There are %s total car images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training car images.' % len(train_files))\n",
    "print('There are %d validation car images.' % len(valid_files))\n",
    "print('There are %d test car images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from tqdm import tqdm\n",
    "def path_to_tensor(img_path):     \n",
    "    # loads RGB image as PIL.Image.Image type     \n",
    "    img = image.load_img(img_path, target_size=(224, 224))     \n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)     \n",
    "    x = image.img_to_array(img)     \n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor     \n",
    "    return np.expand_dims(x, axis=0)  \n",
    "def paths_to_tensor(img_paths):     \n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]     \n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-16a731510e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mImageFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOAD_TRUNCATED_IMAGES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mvalid_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpaths_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_files' is not defined"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "train_tensors = paths_to_tensor(train_files).astype('float64')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
